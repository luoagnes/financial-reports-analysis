我的第一个爬虫----基于selenium和Python2.7
之前一直有想过去自学一下爬虫，但一直因为各种各样更重要的事情给耽误了。这次突然接到图书馆参考部崔老师的任务，要求爬取刚更新的2016年的期刊分区数据，这才匆匆开始自学爬虫。
因为时间只有一周，然后数据量比较大，所以我采用的是直接看网上各位大牛的博客，找到一个相似的例子，稍加修改，争取最短的时间让程序跑起来。但事实并不是像我想的那麽简单，我花了两天时间尝试了网上各种实例，最后都失败了，总结失败的情况如下：
1.	Request
大部分人的例子采用的都是request包，但对于之前完全不了解爬虫机制的我来说，只能生搬硬套。结果都跪了，最主要的原因是在提交网页header时出错。学校的网站是需要登录验证的，但我不知道该像浏览器提交哪些参数，我把整个header都原样提交上去了，发现有时候能登录成功，有时候不行。总是得不到我需要的页面的，我需要的内容在response页面里面，我总是获取不到。还有一个问题是跳转的页面链接和提交表单参数的页面的链接的差异性，及时切换，不然也是得不到想要的结果。
	捣鼓了两天之后实在没办法了，就去跟师弟们交流，有一个师弟之前工作过，所以对于这些也许会有了解。在他的建议下，我开始尝试用selenium包，据说是完全模拟浏览器行为的，操作很方便。事实证明确实如此。但也有几点需要注意：
1.	安装时，除了用pip 安装selenium外，别忘了还要安装对应的浏览器驱动，浏览器驱动的版本应该与你电脑的版本是相对应的。
2.	驱动器的位置不一定要放在特定的目录下，但如果不是默认目录，需要在创建浏览器对象时指明。如：
driver= webdriver.Chrome(executable_path='C:\Python27\chromedriver.exe')
3.	在运行过程中，任何一处小的代码错误都会直接报error，但不会告诉你具体的原因，这时候要注意检查代码逻辑，以及数据一致性。
4.	当涉及到点击页面尾部的超链接时，一定要将页面滚动条拉倒最末尾，使超链接出现在当前鼠标可点击范围内，否则会报错。
5.	部分网站，尤其像学校网站，稳定性非常不好时，每处理一个页面都尽量sleep几秒钟，防止页面转换过快导致网站崩溃或者出错。
6.	如果网页需要在联网条件下才能访问，那么确保程序运行期间网络良好，否则也会老是出错
7.	在访问学校网站时发现当连续登陆2个小时后会自动退出，这时候要设法检测到并及时连接，如果工程量不是很大的情况下可以考虑手动连接。
